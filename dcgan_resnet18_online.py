#!/usr/bin/env python3
#
# Testing whether the GET holds if we are training on inputs from a DCGAN
# pre-trained on CIFAR10, with labels generated by a Resnet18 pre-trained
# on CIFAR10.
#
# Date: December 2020
#
# Author: Sebastian Goldt <goldt.sebastian@gmail.com>

import argparse
import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from dcgan import Generator
from models import ScalarResnet, TwoLayer


def I2_erf(c00, c01, c11):
    return 2.0 / math.pi * torch.asin(c01 / (math.sqrt(1 + c00) * math.sqrt(1 + c11)))


def erfscaled(x):
    return torch.erf(x / math.sqrt(2))


def get_eg_analytical(Q, R, T, A, v):
    """
    Computes the analytical expression for the generalisation error of erf teacher
    and student with the given order parameters.

    Parameters:
    -----------
    Q: student-student overlap
    R: teacher-student overlap
    T: teacher-teacher overlap
    A: teacher second layer weights
    v: student second layer weights
    """
    eg_analytical = 0
    # student-student overlaps
    sqrtQ = torch.sqrt(1 + Q.diag())
    norm = torch.ger(sqrtQ, sqrtQ)
    eg_analytical += torch.sum((v.t() @ v) * torch.asin(Q / norm))
    # teacher-teacher overlaps
    sqrtT = torch.sqrt(1 + T.diag())
    norm = torch.ger(sqrtT, sqrtT)
    eg_analytical += torch.sum((A.t() @ A) * torch.asin(T / norm))
    # student-teacher overlaps
    norm = torch.ger(sqrtQ, sqrtT)
    eg_analytical -= 2.0 * torch.sum((v.t() @ A) * torch.asin(R / norm))
    return eg_analytical / math.pi


def eval_student(time, student, test_loader, test_xs, test_nus, device):
    # N = test_xs.shape[1]

    student.eval()
    with torch.no_grad():
        # compute the generalisation error w.r.t. the noiseless teacher
        eg = 0
        for data, target in test_loader:
            data = data.to(device)
            target = target.to(device)

            preds = student(data)
            eg += F.mse_loss(preds, target)
        eg /= len(test_loader)

        P = test_xs.shape[0]
        w = student.fc1.weight.data.cpu()
        v = student.fc2.weight.data.cpu()
        A = torch.tensor([1]).float()
        lambdas = test_xs @ w.T / math.sqrt(student.N)
        # nus are given to this method
        Q_mc = lambdas.T @ lambdas / P
        R_mc = lambdas.T @ test_nus / P
        T_mc = test_nus.T @ test_nus / P
        eg_analytical = 2 * get_eg_analytical(Q_mc, R_mc, T_mc, A, v)
        gnus = erfscaled(test_nus)
        glambdas = erfscaled(lambdas)
        eg_mc = gnus.T @ gnus / P
        eg_mc += torch.sum((v.T @ v) * (glambdas.T @ glambdas / P))
        eg_mc -= 2 * v @ (glambdas.T @ gnus) / P
        msg = "%g, %g, %g, %g, " % (time, eg, eg_mc, eg_analytical)

    student.train()
    return msg[:-2]


def get_samples(P, D, N, generator, teacher, device):
    """
    Generates a set of test samples.

    Parameters:
    -----------

    scenario : string describing the scenario, e.g. dcgan_rand, nvp_cifar10, ...
    P : number of samples
    D : latent dimension
    N : input dimension
    generator : generative model that transforms latent variables to inputs
    teacher : teacher networks
    """
    with torch.no_grad():
        cs = torch.randn(P, D).to(device)
        latent = cs.unsqueeze(-1).unsqueeze(-1)
        xs = generator(latent)
        # let the teacher act on the 2-D images
        nus, ys = teacher.nu_y(xs)
        xs = xs.reshape(-1, N)

        return cs, xs, nus, ys


def log(msg, logfile):
    """
    Print log message to  stdout and the given logfile.
    """
    print(msg)
    logfile.write(msg + "\n")


def main():
    # define the command line arguments
    bs_help = """
    This is online learning, batch size=1. The batch-size this parameter sets is simply
    the number of samples that are generated or labeled on the GPU in one go.
    """
    device_help = "which device to run on: 'cuda' or 'cpu'"
    steps_help = "number of epochs to train (default: 50)"
    lr_help = "learning rate (default: 0.05)"
    seed_help = "random number generator seed."
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--randomteacher", help="random teacher weights", action="store_true"
    )
    parser.add_argument(
        "--randomgen", help="random teacher weights", action="store_true"
    )
    parser.add_argument("--K", type=int, default=2, help=steps_help)
    parser.add_argument("--lr", type=float, default=0.05, help=lr_help)
    parser.add_argument("--bs", type=int, default=4096, help=bs_help)
    parser.add_argument("--steps", type=int, default=10000, help=steps_help)
    parser.add_argument("--device", "-d", help=device_help)
    parser.add_argument("--dummy", help="dummy option", action="store_true")
    parser.add_argument("-q", "--quiet", help="be quiet", action="store_true")
    parser.add_argument("-s", "--seed", type=int, default=0, help=seed_help)
    args = parser.parse_args()

    torch.manual_seed(args.seed)
    if args.device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)

    # D: generator input dimension
    # N: generator output dimension, student input
    (D, N, M, K, lr, seed) = (100, 3072, 1, args.K, args.lr, args.seed)

    # output file + welcome message
    teacher_desc = "resnet18" + ("rand" if args.randomteacher else "")
    gen_desc = "dcgan" + ("rand" if args.randomgen else "")
    fname_root = "%s_%s_K%d_lr%g_s%d" % (gen_desc, teacher_desc, K, lr, seed)
    logfile = open(fname_root + ".log", "w", buffering=1)
    welcome = "# Checking the GEP for (x=DCGAN, y=Resnet18) on CIFAR10\n"
    welcome += "# Using device:" + str(device)

    # generator
    generator = Generator(ngpu=1)
    # load weights
    if args.randomgen:
        loadweightsfrom = "models/dcgan_rand_weights.pth"
    else:
        loadweightsfrom = "models/dcgan_cifar10_weights.pth"
    generator.load_state_dict(torch.load(loadweightsfrom, map_location=device))
    welcome += "# Loaded pre-trained generator weights from %s\n" % loadweightsfrom
    generator.eval()
    generator.to(device)

    # teacher
    num_classes = 1  # odd-even discrimination
    teacher = ScalarResnet(num_classes, False, False).to(device)
    if not args.randomteacher:
        teacher.load_state_dict(
            torch.load("./models/resnet18_cifar10.pt", map_location=device)
        )
        welcome += (
            "# Loaded pre-trained teacher weights from models/resnet18_cifar10.pt"
        )
    teacher.freeze()
    teacher.eval()
    student = TwoLayer(erfscaled, N, args.K, std0w=1).to(device)
    log(welcome, logfile)

    print("# Teacher and Student: ")
    for net in [teacher, student]:
        msg = "# " + str(net).replace("\n", "\n# ")
        log(msg, logfile)

    # Generate the test set
    test_bs = 40  # test batch size
    batches = 5
    test_cs = torch.zeros((batches * test_bs, D))
    test_xs = torch.zeros((batches * test_bs, 3 * 32 * 32))
    test_nus = torch.zeros((batches * test_bs, 1))
    test_ys = torch.zeros((batches * test_bs, 1))
    print("# Generating test set")
    for idx in range(batches):
        cs, xs, nus, ys = get_samples(
            test_bs,
            D,
            N,
            generator,
            teacher,
            device,
        )
        start = idx * cs.shape[0]
        end = (idx + 1) * cs.shape[0]
        test_cs[start:end] = cs
        test_xs[start:end] = xs
        test_nus[start:end] = nus
        test_ys[start:end] = ys

    # Center the inputs, rescale (these two scalar operations do *not* change the input
    # distribution; they merely amount to choosing the origin and setting the unit
    # length in input space).
    mean, std = torch.mean(test_xs), torch.std(test_xs)
    test_xs = (test_xs - mean) / std

    test_dataset = torch.utils.data.TensorDataset(test_xs, test_ys)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_bs)

    msg = "# test xs: mean=%g, std=%g; test ys: std=%g\n" % (
        torch.mean(test_xs),
        torch.std(test_xs),
        torch.std(test_ys),
    )
    msg += "# test nus: mean=%g, std=%g" % (
        torch.mean(test_nus),
        torch.std(test_nus),
    )
    log(msg, logfile)

    optimizer = torch.optim.SGD(student.parameters(), lr=args.lr)
    criterion = nn.MSELoss()

    # when to print?
    end = torch.log10(torch.tensor([1.0 * args.steps])).item()
    times_to_print = list(torch.logspace(-1, end, steps=200))
    time = 0
    dt = 1 / N

    msg = eval_student(time, student, test_loader, test_xs, test_nus, device)
    log(msg, logfile)

    while len(times_to_print) > 0:
        # get the inputs
        _, inputs, _, targets = get_samples(
            args.bs,
            D,
            N,
            generator,
            teacher,
            device,
        )

        for i in range(args.bs):
            student.train()
            preds = student(inputs[i])
            loss = F.mse_loss(preds, targets[i])

            # TRAINING
            student.zero_grad()
            loss.backward()
            optimizer.step()

            time += dt

            if time >= times_to_print[0].item() or time == 0:
                msg = eval_student(
                    time, student, test_loader, test_xs, test_nus, device
                )
                log(msg, logfile)
                times_to_print.pop(0)

    print("Bye-bye")


if __name__ == "__main__":
    main()
