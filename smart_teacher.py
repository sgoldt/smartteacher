#!/usr/bin/env python3
#
# Testing whether the GET holds if we are training on inputs from a DCGAN
# pre-trained on CIFAR100, with labels generated by a Resnet18 pre-trained
# on CIFAR100.
#
# Date: December 2020
#
# Author: Sebastian Goldt <goldt.sebastian@gmail.com>

import argparse
import math

from tqdm import trange

import torch
import torch.nn.functional as F

from dcgan_cifar100 import Generator
from models import ScalarResnet, TwoLayer, ConvNet, MLP


def I2_erf(c00, c01, c11):
    return 2.0 / math.pi * torch.asin(c01 / (math.sqrt(1 + c00) * math.sqrt(1 + c11)))


def erfscaled(x):
    return torch.erf(x / math.sqrt(2))


def get_eg_analytical(Q, R, T, A, v):
    """
    Computes the analytical expression for the generalisation error of erf teacher
    and student with the given order parameters.

    Parameters:
    -----------
    Q: student-student overlap
    R: teacher-student overlap
    T: teacher-teacher overlap
    A: teacher second layer weights
    v: student second layer weights
    """
    eg_analytical = 0
    # student-student overlaps
    sqrtQ = torch.sqrt(1 + Q.diag())
    norm = torch.ger(sqrtQ, sqrtQ)
    eg_analytical += torch.sum((v.t() @ v) * torch.asin(Q / norm))
    # teacher-teacher overlaps
    sqrtT = torch.sqrt(1 + T.diag())
    norm = torch.ger(sqrtT, sqrtT)
    eg_analytical += torch.sum((A.t() @ A) * torch.asin(T / norm))
    # student-teacher overlaps
    norm = torch.ger(sqrtQ, sqrtT)
    eg_analytical -= 2.0 * torch.sum((v.t() @ A) * torch.asin(R / norm))
    return eg_analytical / math.pi


def eval_student(time, teacher, student, test_loader, test_xs, test_nus, device):
    student.eval()
    with torch.no_grad():
        # compute the generalisation error w.r.t. the noiseless teacher
        eg = 0
        student_std = 0  # std. dev. of student labels
        teacher_std = 0
        for data, target in test_loader:
            data = data.to(device)
            target = target.to(device)

            preds = student(data)
            eg += F.mse_loss(preds, target)
            student_std += torch.std(preds)
            teacher_std += torch.std(target)

        student_std /= len(test_loader)
        teacher_std /= len(test_loader)
        eg /= len(test_loader)

        P = test_xs.shape[0]
        w = student.fc.weight.data.cpu()
        v = student.v.weight.data.cpu()
        A = teacher.v.weight.data.cpu()

        # Calculate the test error using an analytical formula assuming joint Gaussianity
        # of (lambda, nu)
        lambdas = test_xs @ w.T / math.sqrt(student.N)
        # nus are given to this method
        Q_mc = lambdas.T @ lambdas / P
        R_mc = lambdas.T @ test_nus / P
        T_mc = test_nus.T @ test_nus / P
        eg_analytical = 2 * get_eg_analytical(Q_mc, R_mc, T_mc, A, v)

        # Monte Carlo calculation, essentially checking that we have the right variables
        # nu and lambda.
        # gnus = erfscaled(test_nus)
        # glambdas = erfscaled(lambdas)
        # eg_mc = torch.sum((A.T @ A) * (gnus.T @ gnus / P))
        # eg_mc += torch.sum((v.T @ v) * (glambdas.T @ glambdas / P))
        # eg_mc -= 2 * torch.sum((A.T @ v) * (gnus.T @ glambdas / P))
        # msg = ("%g, %g, %g, %g, %g, " % (time, eg, eg_mc, eg_analytical, student_std))

        msg = "%g, %g, %g, %g, %g, " % (
            time,
            eg,
            eg_analytical,
            teacher_std,
            student_std,
        )

    student.train()
    return msg[:-2]


def _get_samples_dcgan(P, D, N, generator, teacher, mean_x, device):
    """
    Generates a set of test samples.

    Parameters:
    -----------

    P : number of samples
    D : latent dimension
    N : input dimension
    generator : generative model that transforms latent variables to inputs
    teacher : teacher networks
    mean_x : the mean of the generator's output
    """
    with torch.no_grad():
        cs = torch.randn(P, D).to(device)
        latent = cs.unsqueeze(-1).unsqueeze(-1)
        xs = generator(latent)
        if teacher.requires_2d_input:
            # let the teacher act on the 2-D images
            nus, ys = teacher.nu_y(xs)
            xs = xs.reshape(-1, N)
            xs -= mean_x
        else:
            xs = xs.reshape(-1, N)
            xs -= mean_x
            nus, ys = teacher.nu_y(xs)

        return cs, xs, nus, ys


def _get_samples_iid(P, D, N, generator, teacher, mean_x, device):
    """
    Generates a set of test samples.

    Parameters:
    -----------

    P : number of samples
    D : latent dimension
    N : input dimension
    generator : generative model that transforms latent variables to inputs
    teacher : teacher networks
    mean_x : the mean of the generator's output
    """
    with torch.no_grad():
        xs = torch.randn(P, N).to(device)
        if teacher.requires_2d_input:
            # let the teacher act on the 2-D images
            xs = xs.reshape(-1, 1, 32, 32)
            nus, ys = teacher.nu_y(xs)
            xs = xs.reshape(-1, N)
        else:
            xs = xs.reshape(-1, N)
            nus, ys = teacher.nu_y(xs)

        return None, xs, nus, ys


def log(msg, logfile):
    """
    Print log message to  stdout and the given logfile.
    """
    print(msg)
    logfile.write(msg + "\n")


def main():
    # define the command line arguments
    bs_help = """
    This is online learning, batch size=1. The batch-size this parameter sets is simply
    the number of samples that are generated or labeled on the GPU in one go.
    """
    teachers = "twolayer | mlp | convnet | resnet18rand"
    generators = "iid | dcgan_rand | dcgan_cifar100_grey"
    device_help = "which device to run on: 'cuda' or 'cpu'"
    steps_help = "number of epochs to train (default: 50)"
    lr_help = "learning rate (default: 0.05)"
    seed_help = "random number generator seed."
    parser = argparse.ArgumentParser()
    parser.add_argument("--teacher", default="twolayer", help=teachers)
    parser.add_argument("--generator", default="dcgan_cifar100_grey", help=generators)
    parser.add_argument("--K", type=int, default=2, help=steps_help)
    parser.add_argument("--lr", type=float, default=0.01, help=lr_help)
    parser.add_argument("--bs", type=int, default=4096, help=bs_help)
    parser.add_argument("--steps", type=int, default=10000, help=steps_help)
    parser.add_argument("--device", "-d", help=device_help)
    parser.add_argument("--dummy", help="dummy option", action="store_true")
    parser.add_argument("-q", "--quiet", help="be quiet", action="store_true")
    parser.add_argument("-s", "--seed", type=int, default=0, help=seed_help)
    args = parser.parse_args()

    torch.manual_seed(args.seed)
    if args.device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)

    # D: generator input dimension
    # N: generator output dimension, student input
    (D, N, M, K, lr, seed) = (100, 1 * 32 * 32, 1, args.K, args.lr, args.seed)

    # initialise the student to guarantee the random weights for the given seed
    student = TwoLayer(N, args.K, std0w=1e-1, std0v=1e-2).to(device)

    welcome = "# Checking the GEP for (x=DCGAN, y=Resnet18) on CIFAR10\n"
    welcome += "# Using device: %s\n" % str(device)

    # generator
    generator = None
    get_samples = None
    # the vectorial mean of the student inputs
    # (i.e. mean_x is a vector, not a quadratic image!)
    mean_x = None
    if args.generator.startswith("dcgan"):
        generator = Generator(ngpu=1)
        get_samples = _get_samples_dcgan

        # find the right weights
        loadweightsfrom = "models/%s_weights.pth" % args.generator
        generator.load_state_dict(torch.load(loadweightsfrom, map_location=device))
        welcome += "# Loaded generator weights from %s\n" % loadweightsfrom
    elif args.generator == "iid":
        get_samples = _get_samples_iid
    else:
        raise ValueError("Wrong generator name (%s)" % generators)

    if generator is None:
        mean_x = torch.zeros(N)
    else:
        generator.eval()
        generator.to(device)
        mean_x = torch.load("models/%s_mean_x.pth" % args.generator)

    # teacher
    num_classes = 1  # odd-even discrimination
    M = None
    if args.teacher == "twolayer":
        M = 2 * args.K
        teacher = TwoLayer(N, M, std0w=1, std0v=1).to(device)
    elif args.teacher == "mlp":
        M = 2 * args.K
        teacher = MLP(N, M).to(device)
    elif args.teacher == "convnet":
        M = 2 * args.K
        teacher = ConvNet(erfscaled, M).to(device)
    elif args.teacher == "resnet18rand":
        M = 1
        teacher = ScalarResnet(num_classes).to(device)
    elif args.teacher == "resnet18cifar100":
        M = 1
        raise NotImplementedError("pre-trained teacher not yet implemented")
        # teacher.load_state_dict(
        #     torch.load("./models/resnet18_cifar10.pt", map_location=device)
        # )
        # welcome += (
        #     "# Loaded pre-trained teacher weights from models/resnet18_cifar10.pt"
        # )
    else:
        raise ValueError("Did not recognise the teacher description.")
    teacher.freeze()
    teacher.eval()

    # output file + welcome message
    fname_root = "%s_%s_K%d_lr%g_s%d" % (args.generator, args.teacher, K, lr, seed)
    logfile = open(fname_root + ".log", "w", buffering=1)
    welcome += "# Teacher and Student:"
    for net in [teacher, student]:
        welcome += "\n# " + str(net).replace("\n", "\n# ")
    log(welcome, logfile)

    # Generate the test set
    test_bs = 1000  # test batch size
    batches = 10
    P = batches * test_bs
    test_cs = torch.zeros((P, D))
    test_xs = torch.zeros((P, N))
    test_nus = torch.zeros((P, M))
    test_ys = torch.zeros((P, 1))
    print("# Generating test set")
    for idx in trange(batches):
        cs, xs, nus, ys = get_samples(
            test_bs,
            D,
            N,
            generator,
            teacher,
            mean_x,
            device,
        )
        start = idx * xs.shape[0]
        end = (idx + 1) * xs.shape[0]
        if cs is not None:
            test_cs[start:end] = cs
        test_xs[start:end] = xs
        test_nus[start:end] = nus
        test_ys[start:end] = ys

    test_dataset = torch.utils.data.TensorDataset(test_xs, test_ys)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_bs)

    msg = "# test xs: mean=%g, std=%g; test ys: std=%g\n" % (
        torch.mean(test_xs),
        torch.std(test_xs),
        torch.std(test_ys),
    )
    msg += "# test nus: mean=%g, std=%g" % (
        torch.mean(test_nus),
        torch.std(test_nus),
    )
    log(msg, logfile)

    optimizer = torch.optim.SGD(student.parameters(), lr=args.lr)

    # when to print?
    end = torch.log10(torch.tensor([1.0 * args.steps])).item()
    times_to_print = list(torch.logspace(-1, end, steps=200))
    time = 0
    dt = 1 / N

    msg = eval_student(time, teacher, student, test_loader, test_xs, test_nus, device)
    log(msg, logfile)

    while len(times_to_print) > 0:
        # get the inputs
        _, inputs, _, targets = get_samples(
            args.bs,
            D,
            N,
            generator,
            teacher,
            mean_x,
            device,
        )

        for i in range(args.bs):
            student.train()
            preds = student(inputs[i])
            loss = F.mse_loss(preds, targets[i])

            # TRAINING
            student.zero_grad()
            loss.backward()
            optimizer.step()

            time += dt

            if time >= times_to_print[0].item() or time == 0:
                msg = eval_student(
                    time, teacher, student, test_loader, test_xs, test_nus, device
                )
                log(msg, logfile)
                times_to_print.pop(0)

    print("Bye-bye")


if __name__ == "__main__":
    main()
