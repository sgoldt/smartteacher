#!/usr/bin/env python3
#
# Testing whether the GET holds if we are training on inputs from a DCGAN
# pre-trained on CIFAR100, with labels generated by a Resnet18 pre-trained
# on CIFAR100.
#
# Date: December 2020
#
# Author: Sebastian Goldt <goldt.sebastian@gmail.com>

import argparse
import math

from tqdm import trange

import torch
import torch.nn.functional as F

from dcgan_cifar10 import Generator
from models import ScalarResnet, TwoLayer, ConvNet, MLP
import utils


def eval_student(time, teacher, student, test_loader, test_xs, test_nus, device):
    student.eval()
    with torch.no_grad():
        # compute the generalisation error w.r.t. the noiseless teacher
        eg = 0
        student_std = 0  # std. dev. of student labels
        teacher_std = 0
        for data, target in test_loader:
            data = data.to(device)
            target = target.to(device)

            preds = student(data)
            eg += F.mse_loss(preds, target)
            student_std += torch.std(preds)
            teacher_std += torch.std(target)

        student_std /= len(test_loader)
        teacher_std /= len(test_loader)
        eg /= len(test_loader)

        P = test_xs.shape[0]
        w = student.fc.weight.data.cpu()
        v = student.v.weight.data.cpu()
        A = teacher.v.weight.data.cpu()

        # Calculate the test error using an analytical formula assuming joint Gaussianity
        # of (lambda, nu)
        lambdas = test_xs @ w.T / math.sqrt(student.input_dim)
        # nus are given to this method
        Q_mc = lambdas.T @ lambdas / P
        R_mc = lambdas.T @ test_nus / P
        T_mc = test_nus.T @ test_nus / P
        eg_analytical = 2 * utils.get_eg_analytical(Q_mc, R_mc, T_mc, A, v)

        # Monte Carlo calculation, essentially checking that we have the right variables
        # nu and lambda.
        # gnus = erfscaled(test_nus)
        # glambdas = erfscaled(lambdas)
        # eg_mc = torch.sum((A.T @ A) * (gnus.T @ gnus / P))
        # eg_mc += torch.sum((v.T @ v) * (glambdas.T @ glambdas / P))
        # eg_mc -= 2 * torch.sum((A.T @ v) * (gnus.T @ glambdas / P))
        # msg = ("%g, %g, %g, %g, %g, " % (time, eg, eg_mc, eg_analytical, student_std))

        msg = "%g, %g, %g, %g, %g, " % (
            time,
            eg,
            eg_analytical,
            teacher_std,
            student_std,
        )

    student.train()
    return msg[:-2]


def _get_samples_dcgan(P, D, N, generator, teacher, mean_x, device):
    """
    Generates a set of test samples.

    Parameters:
    -----------

    P : number of samples
    D : latent dimension
    N : input dimension
    generator : generative model that transforms latent variables to inputs
    teacher : teacher networks
    mean_x : the mean of the generator's output
    """
    with torch.no_grad():
        cs = torch.randn(P, D).to(device)
        latent = cs.unsqueeze(-1).unsqueeze(-1)
        xs = generator(latent)
        if teacher.requires_2d_input:
            # let the teacher act on the 2-D images
            nus, ys = teacher.nu_y(xs)
            xs = xs.reshape(-1, N)
            xs -= mean_x
        else:
            xs = xs.reshape(-1, N)
            xs -= mean_x
            nus, ys = teacher.nu_y(xs)

        return cs, xs, nus, ys


def _get_samples_iid(P, D, N, generator, teacher, mean_x, device):
    """
    Generates a set of test samples.

    Parameters:
    -----------

    P : number of samples
    D : latent dimension
    N : input dimension
    generator : generative model that transforms latent variables to inputs
    teacher : teacher networks
    mean_x : the mean of the generator's output
    """
    with torch.no_grad():
        xs = torch.randn(P, N).to(device)
        if teacher.requires_2d_input:
            # let the teacher act on the 2-D images
            xs = xs.reshape(-1, 1, 32, 32)
            nus, ys = teacher.nu_y(xs)
            xs = xs.reshape(-1, N)
        else:
            xs = xs.reshape(-1, N)
            nus, ys = teacher.nu_y(xs)

        return None, xs, nus, ys


def log(msg, logfile):
    """
    Print log message to  stdout and the given logfile.
    """
    print(msg)
    logfile.write(msg + "\n")


def main():
    # define the command line arguments
    bs_help = """
    This is online learning, batch size=1. The batch-size this parameter sets is simply
    the number of samples that are generated or labeled on the GPU in one go.
    """
    device_help = "which device to run on: 'cuda' or 'cpu'"
    steps_help = "number of epochs to train (default: 50)"
    lr_help = "learning rate (default: 0.05)"
    seed_help = "random number generator seed."
    parser = argparse.ArgumentParser()
    parser.add_argument("--teacher", default="twolayer", help=utils.teachers)
    parser.add_argument(
        "--dataset", default="rand", help="teacher weights: rand | cifar10"
    )
    parser.add_argument("--generator", default="dcgan_cifar10", help=utils.generators)
    parser.add_argument("--K", type=int, default=2, help=steps_help)
    parser.add_argument("--lr", type=float, default=0.01, help=lr_help)
    parser.add_argument("--bs", type=int, default=4096, help=bs_help)
    parser.add_argument("--steps", type=int, default=10000, help=steps_help)
    parser.add_argument("--device", "-d", help=device_help)
    parser.add_argument("--dummy", help="dummy option", action="store_true")
    parser.add_argument("-q", "--quiet", help="be quiet", action="store_true")
    parser.add_argument("-s", "--seed", type=int, default=0, help=seed_help)
    args = parser.parse_args()

    torch.manual_seed(args.seed)
    if args.device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)

    # D: generator input dimension
    # N: generator output dimension, student input
    num_channels = 1 if ("gray" in args.dataset) else 3
    (D, N, M, K, lr, seed) = (
        100,
        num_channels * 32 * 32,
        1,
        args.K,
        args.lr,
        args.seed,
    )

    # initialise the student to guarantee the random weights for the given seed
    student = TwoLayer(N, args.K, std0w=1e-1, std0v=1e-2).to(device)

    welcome = "# Checking the GEP for (x=%s, y=%s)\n" % (args.generator, args.teacher)
    welcome += "# Using device: %s\n" % str(device)

    # generator
    generator = None
    get_samples = None
    # the vectorial mean of the student inputs
    # (i.e. mean_x is a vector, not a quadratic image!)
    mean_x = None
    if args.generator.startswith("dcgan"):
        generator = Generator(ngpu=1)
        get_samples = _get_samples_dcgan

        # find the right weights
        loadweightsfrom = "models/%s_weights.pth" % args.generator
        generator.load_state_dict(torch.load(loadweightsfrom, map_location=device))
        welcome += "# Loaded generator weights from %s\n" % loadweightsfrom
    elif args.generator == "iid":
        get_samples = _get_samples_iid
    else:
        raise ValueError("Wrong generator name (%s)" % generators)

    if generator is None:
        mean_x = torch.zeros(N)
    else:
        generator.eval()
        generator.to(device)
        fname = "models/%s_mean_x.pth" % args.generator
        mean_x = torch.load(fname, map_location=device)

    # teacher
    M = {"twolayer": 2 * args.K, "mlp": 1, "convnet": 2 * args.K, "resnet18": 1}[
        args.teacher
    ]
    teacher = utils.get_teacher(
        args.teacher, N, M
    )
    teacher_weights_fname = "rand"
    if args.dataset != "rand":
        teacher_weights_fname = "./models/%s_%s.pth" % (args.teacher, args.dataset)
        teacher.load_state_dict(
            torch.load(teacher_weights_fname, map_location=device)
        )
    welcome += "# Teacher weights: %s\n" % teacher_weights_fname
    teacher.to(device)
    teacher.freeze()
    teacher.eval()

    # output file + welcome message
    dataset_desc = "rand" if args.dataset is None else args.dataset
    fname_root = "%s_%s_K%d_%s_lr%g_s%d" % (
        args.generator,
        args.teacher,
        K,
        dataset_desc,
        lr,
        seed,
    )
    logfile = open(fname_root + ".log", "w", buffering=1)
    welcome += "# Generator, teacher and Student:"
    for net in [generator, teacher, student]:
        if net is not None:
            welcome += "\n# " + str(net).replace("\n", "\n# ")
    log(welcome, logfile)

    # Generate the test set
    test_bs = 1000  # test batch size
    batches = 10
    P = batches * test_bs
    test_cs = torch.zeros((P, D))
    test_xs = torch.zeros((P, N))
    test_nus = torch.zeros((P, M))
    test_ys = torch.zeros((P, 1))
    print("# Generating test set")
    for idx in trange(batches):
        cs, xs, nus, ys = get_samples(
            test_bs,
            D,
            N,
            generator,
            teacher,
            mean_x,
            device,
        )
        start = idx * xs.shape[0]
        end = (idx + 1) * xs.shape[0]
        if cs is not None:
            test_cs[start:end] = cs
        test_xs[start:end] = xs
        test_nus[start:end] = nus
        test_ys[start:end] = ys

    test_dataset = torch.utils.data.TensorDataset(test_xs, test_ys)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_bs)

    msg = "# test xs: mean=%g, std=%g; test ys: std=%g\n" % (
        torch.mean(test_xs),
        torch.std(test_xs),
        torch.std(test_ys),
    )
    msg += "# test nus: mean=%g, std=%g" % (
        torch.mean(test_nus),
        torch.std(test_nus),
    )
    log(msg, logfile)

    optimizer = torch.optim.SGD(student.parameters(), lr=args.lr)

    # when to print?
    end = torch.log10(torch.tensor([1.0 * args.steps])).item()
    times_to_print = list(torch.logspace(-1, end, steps=200))
    time = 0
    dt = 1 / N

    msg = eval_student(time, teacher, student, test_loader, test_xs, test_nus, device)
    log(msg, logfile)

    while len(times_to_print) > 0:
        # get the inputs
        _, inputs, _, targets = get_samples(
            args.bs,
            D,
            N,
            generator,
            teacher,
            mean_x,
            device,
        )

        for i in range(args.bs):
            student.train()
            preds = student(inputs[i])
            loss = F.mse_loss(preds, targets[i])

            # TRAINING
            student.zero_grad()
            loss.backward()
            optimizer.step()

            time += dt

            if time >= times_to_print[0].item() or time == 0:
                msg = eval_student(
                    time, teacher, student, test_loader, test_xs, test_nus, device
                )
                log(msg, logfile)
                times_to_print.pop(0)

    print("Bye-bye")


if __name__ == "__main__":
    main()
